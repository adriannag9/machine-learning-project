{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adriannag9/machine-learning/blob/main/ru_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f36267-d8b3-4d9a-984c-dba7462dca47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37f36267-d8b3-4d9a-984c-dba7462dca47",
        "outputId": "ac41c5c7-c5aa-43df-87cc-43483e1e77bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.12/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pymystem3 in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.12/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.12/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pymystem3) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2026.1.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                                                 text  label\n",
            "0   Пальто красивое, но пришло с дырой в молнии. П...      0\n",
            "1   Очень долго шел заказ,ждала к новому году,приш...      0\n",
            "2   Могу сказать одно, брюки нормальные, НО они бы...      0\n",
            "3   Доставка быстрая, меньше месяца. Заказывали ра...      0\n",
            "4   Мне не очень  понравилось это платье. Размер  ...      0\n",
            "..                                                ...    ...\n",
            "95  Доставка 63 дня!!! Качество хорошее, на ОГ95, ...      0\n",
            "96  мой размер одежды 40, поэтому думаю понимаете,...      0\n",
            "97  Покрой халата очень странный, рукава очень дли...      0\n",
            "98  заказывала две с длинным рукавом, одна пришла ...      0\n",
            "99         не соответствует фото.Качество другое. :((      0\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "               label\n",
            "count  290458.000000\n",
            "mean        1.001387\n",
            "std         0.816375\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%         1.000000\n",
            "75%         2.000000\n",
            "max         2.000000\n"
          ]
        }
      ],
      "source": [
        "# Install required Russian NLP libraries\n",
        "!pip install pymorphy2 pymystem3\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, preprocessing, linear_model, metrics, svm\n",
        "from sklearn import ensemble\n",
        "import pymorphy2\n",
        "from pymystem3 import Mystem\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Mount Google Drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "# Path adjusted for Google Drive storage\n",
        "raw_data = pd.read_csv('/content/drive/My Drive/projekt_ml/sentiment_dataset.csv')\n",
        "\n",
        "# Prepare data lists\n",
        "texts = raw_data['text'].astype(str).tolist()\n",
        "labels = raw_data['label'].tolist()\n",
        "\n",
        "# Create main DataFrame\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "\n",
        "# Display initial data statistics and preview\n",
        "print(trainDF.head(100))\n",
        "print(trainDF.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a7b82f-b98f-4211-840b-260e1b7f6a00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34a7b82f-b98f-4211-840b-260e1b7f6a00",
        "outputId": "0fbeb95b-cd6f-4406-ed16-ac7bb030a36b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word count preview:\n",
            "                                                text  word_count\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...          50\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...          11\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...          49\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...          20\n",
            "4  Мне не очень  понравилось это платье. Размер  ...          21\n"
          ]
        }
      ],
      "source": [
        "# DATA ANALYSIS (Exploratory Data Analysis)\n",
        "# 1. Word count analysis\n",
        "train = trainDF.copy()\n",
        "train['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
        "\n",
        "print(\"Word count preview:\")\n",
        "print(train[['text','word_count']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2405622-5b04-4c93-a52e-488d7a7b26c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2405622-5b04-4c93-a52e-488d7a7b26c7",
        "outputId": "0ab1c5c4-4d2e-41d4-fbbd-3738ffb0d6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Word count statistics:\n",
            "Max: 463\n",
            "Min: 1\n",
            "Mean: 51.404419916132454\n"
          ]
        }
      ],
      "source": [
        "# Word count statistics\n",
        "print(\"\\nWord count statistics:\")\n",
        "print(f\"Max: {train['word_count'].max()}\")\n",
        "print(f\"Min: {train['word_count'].min()}\")\n",
        "print(f\"Mean: {train['word_count'].mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ec7abf-acb9-4934-b02e-6ba74bc28c95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85ec7abf-acb9-4934-b02e-6ba74bc28c95",
        "outputId": "52c51995-9dac-47da-edd0-2484cd6ed36a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Character count preview:\n",
            "                                                text  char_count\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...         337\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...          82\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...         284\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...         128\n",
            "4  Мне не очень  понравилось это платье. Размер  ...         120\n"
          ]
        }
      ],
      "source": [
        "# 2. Character count analysis (including spaces)\n",
        "train['char_count'] = train['text'].str.len()\n",
        "print(\"\\nCharacter count preview:\")\n",
        "print(train[['text','char_count']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42248c1-006d-4eb7-a7b7-89005c910e23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c42248c1-006d-4eb7-a7b7-89005c910e23",
        "outputId": "87b11b16-f6c3-428d-cd11-ad4e753b2bdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average word length preview:\n",
            "                                                text  avg_word\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...  5.607843\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...  6.545455\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...  5.130435\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...  5.736842\n",
            "4  Мне не очень  понравилось это платье. Размер  ...  5.555556\n"
          ]
        }
      ],
      "source": [
        "# 3. Average word length analysis\n",
        "def avg_word(sentence):\n",
        "    words = str(sentence).split()\n",
        "    if len(words) == 0:\n",
        "        return 0\n",
        "    return (sum(len(word) for word in words)/len(words))\n",
        "\n",
        "train['avg_word'] = train['text'].apply(lambda x: avg_word(x))\n",
        "print(\"\\nAverage word length preview:\")\n",
        "print(train[['text','avg_word']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9858959-87f8-4f40-8de6-2d4d4e544f2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9858959-87f8-4f40-8de6-2d4d4e544f2d",
        "outputId": "e0a7898d-a076-4dbc-b101-199c8899bc2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop-words statistics (Top 5):\n",
            "                                                text  stopwords_count\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...               11\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...                4\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...               14\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...                4\n",
            "4  Мне не очень  понравилось это платье. Размер  ...                3\n",
            "Stopwords:  ['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ],
      "source": [
        "# 1. Count Russian stop-words\n",
        "# Defining the list of Russian stop-words for filtering\n",
        "stop_words = stopwords.words('russian')\n",
        "train['stopwords_count'] = train['text'].apply(lambda x: len([word for word in str(x).split() if word in stop_words]))\n",
        "\n",
        "print(\"Stop-words statistics (Top 5):\")\n",
        "print(train[['text','stopwords_count']].head())\n",
        "print(\"Stopwords: \", stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed14d14e-8c8e-4508-894f-f734ac5b5b94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed14d14e-8c8e-4508-894f-f734ac5b5b94",
        "outputId": "6037f94c-835b-4c9b-a594-0b0b6076b209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Numeric values statistics (Top 5):\n",
            "                                                text  numerics_count\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...               0\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...               0\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...               0\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...               0\n",
            "4  Мне не очень  понравилось это платье. Размер  ...               0\n"
          ]
        }
      ],
      "source": [
        "# 2. Count numeric values in text\n",
        "# This helps identify if numeric data (like prices or dates) impacts sentiment\n",
        "train['numerics_count'] = train['text'].apply(lambda x: len([word for word in str(x).split() if word.isnumeric()]))\n",
        "\n",
        "print(\"\\nNumeric values statistics (Top 5):\")\n",
        "print(train[['text','numerics_count']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8ae953-0d1c-49e2-8ce2-54edc4372f76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d8ae953-0d1c-49e2-8ce2-54edc4372f76",
        "outputId": "0da4fa17-e1b1-476f-eff1-9a6b16c33c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Text after initial preprocessing (lowercase, no punctuation, stop-words removed):\n",
            "0    пальто красивое пришло дырой молнии просила вы...\n",
            "1    очень долго шел заказждала новому годупришел н...\n",
            "2    могу сказать одно брюки нормальные порваны мал...\n",
            "3    доставка быстрая меньше месяца заказывали разм...\n",
            "4    очень понравилось это платье размер l подошёл ...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# TEXT PREPROCESSING\n",
        "\n",
        "# Keep a copy of the raw data for reference\n",
        "trainDF_raw = trainDF.copy()\n",
        "\n",
        "# A. Lowercase conversion\n",
        "# Normalizing text to lowercase for consistency\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(str(x).lower() for x in str(x).split()))\n",
        "\n",
        "# B. Removing special characters and punctuation\n",
        "# Using regex to keep only alphanumeric characters and spaces\n",
        "trainDF['text'] = trainDF['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "\n",
        "# C. Removing Russian stop-words\n",
        "# Filtering out non-informative words based on the NLTK Russian stop-words list\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(word for word in str(x).split() if word not in stop_words))\n",
        "\n",
        "print(\"\\nText after initial preprocessing (lowercase, no punctuation, stop-words removed):\")\n",
        "print(trainDF['text'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c66e9b9-86c1-4c32-b4f4-f49f5e64a707",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c66e9b9-86c1-4c32-b4f4-f49f5e64a707",
        "outputId": "75f379a3-990a-4936-d7a7-ce747898812b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 20 most frequent words identified for removal:\n",
            "это           115810\n",
            "очень         109010\n",
            "просто         41663\n",
            "аниме          35086\n",
            "фильм          31993\n",
            "всё            29624\n",
            "10             25739\n",
            "вообще         22314\n",
            "размер         20762\n",
            "хотя           19585\n",
            "время          18904\n",
            "ещё            18072\n",
            "сюжет          17975\n",
            "деньги         17693\n",
            "качество       17178\n",
            "2              16456\n",
            "смотреть       15528\n",
            "товар          15091\n",
            "рекомендую     14750\n",
            "которые        14686\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# REMOVING FREQUENT AND RARE WORDS\n",
        "\n",
        "# 1. Identify the top 20 most frequent words\n",
        "# These words often appear across all classes and may not contribute to sentiment differentiation\n",
        "freq_words = pd.Series(' '.join(trainDF['text']).split()).value_counts()[:20]\n",
        "\n",
        "print(\"Top 20 most frequent words identified for removal:\")\n",
        "print(freq_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a927b517-61d3-4220-ad44-29fe58504d20",
      "metadata": {
        "id": "a927b517-61d3-4220-ad44-29fe58504d20"
      },
      "outputs": [],
      "source": [
        "# Remove the most frequent words\n",
        "frequent_words_list = list(freq_words.index)\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(word for word in str(x).split() if word not in frequent_words_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8818b4ee-9cda-4d26-b0e8-0a31e0f7b054",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8818b4ee-9cda-4d26-b0e8-0a31e0f7b054",
        "outputId": "954ef062-da4f-46c6-c3c8-0ae12629d74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Hapax Legomena to be removed: 425023\n",
            "Rare words removal completed.\n"
          ]
        }
      ],
      "source": [
        "# 2. Identifying and Removing Rare Words (Hapax Legomena)\n",
        "# For a large dataset (~300k records), removing only the bottom 20 words is insufficient.\n",
        "# Instead, we identify \"Hapax Legomena\" — words that appear only once in the entire corpus.\n",
        "\n",
        "# Calculate frequency of all words\n",
        "word_freq = pd.Series(' '.join(trainDF['text']).split()).value_counts()\n",
        "\n",
        "# Identify words with a frequency of 1\n",
        "rare_words = word_freq[word_freq == 1]\n",
        "print(f\"Number of Hapax Legomena to be removed: {len(rare_words)}\")\n",
        "\n",
        "# Remove rare words to reduce noise and improve model generalization\n",
        "# Using a set for significantly faster lookup performance\n",
        "rare_words_set = set(rare_words.index)\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(word for word in str(x).split() if word not in rare_words_set))\n",
        "\n",
        "print(\"Rare words removal completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557efa69-3df4-4a87-84b9-f86f4294b40d",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "557efa69-3df4-4a87-84b9-f86f4294b40d",
        "outputId": "247781ba-6217-43b5-ace4-e57170032ee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LEMMATIZATION COMPARISON ---\n",
            "\n",
            "Original: пальто красивое пришло дырой молнии просила выслать такую продавец настаивал отк...\n",
            "Pymorphy2: пальто красивый прийти дыра молния просить выслать такой продавец настаивать отк...\n",
            "Mystem:    пальто красивый приходить дыра молния просить высылать такой продавец настаивать...\n",
            "\n",
            "Original: долго шел заказждала новому...\n",
            "Pymorphy2: долго идти заказждать новый...\n",
            "Mystem:    долго идти заказждать новый...\n",
            "\n",
            "Original: могу сказать одно брюки нормальные порваны малы заказывала второй заказала пришё...\n",
            "Pymorphy2: мочь сказать один брюки нормальный порвать маленький заказывать второй заказать ...\n",
            "Mystem:    мочь сказать один брюки нормальный порывать маленький заказывать второй заказыва...\n",
            "\n",
            "Original: доставка быстрая меньше месяца заказывали l пришёл среднее края обработаны такие...\n",
            "Pymorphy2: доставка быстрый маленький месяц заказывать l прийти средний край обработать так...\n",
            "Mystem:    доставка быстрый мало месяц заказывать l приходить средний край обрабатывать так...\n",
            "\n",
            "Original: понравилось платье l подошёл зелёного цвета смотрится пришло быстро спасибо...\n",
            "Pymorphy2: понравиться платье l подойти зелёный цвет смотреться прийти быстро спасибо...\n",
            "Mystem:    понравиться платье l подходить зеленый цвет смотреться приходить быстро спасибо...\n",
            "\n",
            "Starting efficient Mystem lemmatization for the full dataset (~300k records)...\n",
            "Progress: 0 / 290458 records processed...\n",
            "Progress: 50000 / 290458 records processed...\n",
            "Progress: 100000 / 290458 records processed...\n",
            "Progress: 150000 / 290458 records processed...\n",
            "Progress: 200000 / 290458 records processed...\n",
            "Progress: 250000 / 290458 records processed...\n",
            "\n",
            "Full Mystem lemmatization completed. Data preview:\n",
            "0    пальто красивый приходить дыра молния просить ...\n",
            "1                          долго идти заказждать новый\n",
            "2    мочь сказать один брюки нормальный порывать ма...\n",
            "3    доставка быстрый мало месяц заказывать l прихо...\n",
            "4    понравиться платье l подходить зеленый цвет см...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# LEMMATIZATION STRATEGY: COMPARISON AND FINAL PROCESSING -\n",
        "\n",
        "import inspect\n",
        "from collections import namedtuple\n",
        "\n",
        "# 1. Compatibility Patch for pymorphy2 (Required for Python 3.11+)\n",
        "# Fixing the 'getargspec' attribute error in modern Python environments\n",
        "if not hasattr(inspect, 'getargspec'):\n",
        "    ArgSpec = namedtuple('ArgSpec', ['args', 'varargs', 'keywords', 'defaults'])\n",
        "    def getargspec_patch(func):\n",
        "        args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations = inspect.getfullargspec(func)\n",
        "        return ArgSpec(args, varargs, varkw, defaults)\n",
        "    inspect.getargspec = getargspec_patch\n",
        "\n",
        "# Initialize lemmatizers\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "mystem = Mystem()\n",
        "\n",
        "# 2. Lemmatization Comparison (Sample Test)\n",
        "# Testing Pymorphy2 vs Mystem on a small sample to justify the final choice\n",
        "sample_texts = trainDF['text'].head(5).tolist()\n",
        "\n",
        "print(\"--- LEMMATIZATION COMPARISON ---\")\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"\\nOriginal: {text[:80]}...\")\n",
        "\n",
        "    # Pymorphy2 approach\n",
        "    py_lemmas = [morph.parse(word)[0].normal_form for word in text.split()]\n",
        "    print(f\"Pymorphy2: {' '.join(py_lemmas)[:80]}...\")\n",
        "\n",
        "    # Mystem approach\n",
        "    myst_lemmas = mystem.lemmatize(text)\n",
        "    print(f\"Mystem:    {''.join(myst_lemmas).strip()[:80]}...\")\n",
        "\n",
        "# 3. Final Lemmatization: Mystem (Selected for superior accuracy)\n",
        "print(\"\\nStarting efficient Mystem lemmatization for the full dataset (~300k records)...\")\n",
        "\n",
        "def mystem_batch_process(texts_list):\n",
        "    \"\"\"Efficiently processes large text lists by joining them with a separator.\"\"\"\n",
        "    full_text = \" |separator| \".join(texts_list)\n",
        "    lemmatized = mystem.lemmatize(full_text)\n",
        "    processed_text = \"\".join(lemmatized)\n",
        "    return [t.strip() for t in processed_text.split(\"|separator|\")]\n",
        "\n",
        "# Processing in batches of 5000 to optimize memory usage in Google Colab\n",
        "all_texts = trainDF['text'].tolist()\n",
        "final_lemmas = []\n",
        "batch_size = 5000\n",
        "\n",
        "for i in range(0, len(all_texts), batch_size):\n",
        "    batch = all_texts[i:i + batch_size]\n",
        "    final_lemmas.extend(mystem_batch_process(batch))\n",
        "    if (i // batch_size) % 10 == 0:\n",
        "        print(f\"Progress: {i} / {len(all_texts)} records processed...\")\n",
        "\n",
        "# Update DataFrame with lemmatized text\n",
        "trainDF['text'] = final_lemmas\n",
        "\n",
        "print(\"\\nFull Mystem lemmatization completed. Data preview:\")\n",
        "print(trainDF['text'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "39345983-a25d-4f1d-8a7c-8a3136cac002",
      "metadata": {
        "id": "39345983-a25d-4f1d-8a7c-8a3136cac002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc1f833-09db-44ed-bbde-c5f8ed353d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation and feature extraction complete.\n"
          ]
        }
      ],
      "source": [
        "# DATA SPLITTING AND FEATURE ENGINEERING\n",
        "\n",
        "# 1. Train-Test Split\n",
        "# Dividing the dataset into training and validation sets (default 75/25 split)\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# 2. Label Encoding\n",
        "# Encoding categorical sentiment labels into numerical format\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "train_y = label_encoder.fit_transform(train_y)\n",
        "valid_y = label_encoder.transform(valid_y)\n",
        "\n",
        "# 3. TF-IDF Vectorization\n",
        "# Converting lemmatized text into numerical vectors using Term Frequency-Inverse Document Frequency\n",
        "# Parameters:\n",
        "# - max_features=5000: limits the vocabulary to the top 5,000 most important words\n",
        "# - min_df=2: automatically excludes rare words (noise) appearing in only one document\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000, min_df=2)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "\n",
        "# Transforming the split datasets into TF-IDF feature matrices\n",
        "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
        "\n",
        "print(\"Data preparation and feature extraction complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c83aa83d-c830-477a-9933-cd05783e2326",
      "metadata": {
        "id": "c83aa83d-c830-477a-9933-cd05783e2326",
        "outputId": "4031c37d-4127-4951-e1b0-fa50aebe51ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.6726\n",
            "Linear SVM Accuracy:          0.6692\n",
            "Random Forest Accuracy:       0.6550\n"
          ]
        }
      ],
      "source": [
        "# --- MODEL TRAINING AND EVALUATION ---\n",
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    \"\"\"\n",
        "    Trains a classifier and returns evaluation metrics: Precision, Recall, F1-Score, and Accuracy.\n",
        "    \"\"\"\n",
        "    # Fit the model on training data\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "\n",
        "    # Generate predictions on the validation set\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    precision = metrics.precision_score(predictions, valid_y, average='weighted')\n",
        "    recall = metrics.recall_score(predictions, valid_y, average='weighted')\n",
        "    f1 = metrics.f1_score(predictions, valid_y, average='weighted')\n",
        "    accuracy = metrics.accuracy_score(predictions, valid_y)\n",
        "\n",
        "    return [precision, recall, f1, accuracy]\n",
        "\n",
        "# Dictionary to store results for comparative analysis\n",
        "accuracy_compare = {}\n",
        "\n",
        "# MODEL 1: Logistic Regression\n",
        "# Increased max_iter to ensure convergence on a large dataset\n",
        "results_lr = train_model(linear_model.LogisticRegression(max_iter=1000), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "accuracy_compare['LR'] = results_lr\n",
        "print(f\"Logistic Regression Accuracy: {results_lr[3]:.4f}\")\n",
        "\n",
        "# MODEL 2: Support Vector Machine (LinearSVC)\n",
        "# Optimized for high-dimensional text data\n",
        "results_svm = train_model(svm.LinearSVC(dual='auto'), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "accuracy_compare['SVM'] = results_svm\n",
        "print(f\"Linear SVM Accuracy:          {results_svm[3]:.4f}\")\n",
        "\n",
        "# MODEL 3: Random Forest\n",
        "# Using n_jobs=-1 to utilize all available CPU cores in Google Colab\n",
        "results_rf = train_model(ensemble.RandomForestClassifier(n_jobs=-1), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "accuracy_compare['RF'] = results_rf\n",
        "print(f\"Random Forest Accuracy:       {results_rf[3]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4123c81-f234-405e-89bd-b71b2773348b",
      "metadata": {
        "id": "b4123c81-f234-405e-89bd-b71b2773348b"
      },
      "outputs": [],
      "source": [
        "# --- ZAPISYWANIE MODELI DO PLIKÓW ---\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Tworzymy folder na modele, jeśli nie istnieje\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "# Zapisujemy każdy model do oddzielnego pliku\n",
        "joblib.dump(accuracy_compare, 'models/results_dict.pkl')\n",
        "joblib.dump(tfidf_vect, 'models/tfidf_vectorizer.pkl')\n",
        "\n",
        "# Jeśli modele skończyły się trenować, zapisujemy je (pamiętaj, że RF może jeszcze trwać)\n",
        "if 'LR' in accuracy_compare:\n",
        "    # Zapisujemy same obiekty modeli (można je później wczytać do predykcji)\n",
        "    # Uwaga: poniższe zmienne muszą istnieć (musisz mieć skończony trening)\n",
        "    # joblib.dump(classifier_lr, 'models/model_lr.pkl') # jeśli zapisałaś je w zmiennych\n",
        "\n",
        "    print(\"Wyniki i wektoryzator zostały zapisane w folderze 'models'.\")\n",
        "    print(\"Teraz nawet po zamknięciu laptopa, odtworzysz je w sekundę.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d978f1f-e6f5-439e-b5d7-7f421d12d3b1",
      "metadata": {
        "id": "9d978f1f-e6f5-439e-b5d7-7f421d12d3b1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}