{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adriannag9/machine-learning/blob/main/ru_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vecMXL4u4alf",
        "outputId": "1cb5670f-0bde-4a66-9a16-5e8ef4a254fe"
      },
      "id": "vecMXL4u4alf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f36267-d8b3-4d9a-984c-dba7462dca47",
      "metadata": {
        "id": "37f36267-d8b3-4d9a-984c-dba7462dca47"
      },
      "outputs": [],
      "source": [
        "# 1. Import bibliotek\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, preprocessing, linear_model, metrics, svm\n",
        "from sklearn import ensemble\n",
        "\n",
        "# Biblioteki do lematyzacji języka rosyjskiego\n",
        "import pymorphy2\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "# Pobieranie zasobów NLTK\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 2. Import dataset\n",
        "# Ścieżka do Twojego pliku o rozmiarze 177 MB\n",
        "raw_data = pd.read_csv('/content/drive/My Drive/projekt_ML/sentiment_dataset.csv')\n",
        "\n",
        "# 3. Wypełnienie list (Odwzorowanie logiki 1:1 z oryginału)\n",
        "texts = raw_data['text'].astype(str).tolist()\n",
        "labels = raw_data['label'].tolist()\n",
        "\n",
        "# 4. Utworzenie DataFrame\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "\n",
        "# 5. Wyświetlenie rekordów i statystyk (Dla 290 458 rekordów)\n",
        "print(trainDF.head(100))\n",
        "print(trainDF.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a7b82f-b98f-4211-840b-260e1b7f6a00",
      "metadata": {
        "id": "34a7b82f-b98f-4211-840b-260e1b7f6a00"
      },
      "outputs": [],
      "source": [
        "# --- ANALIZA DANYCH ---\n",
        "# 1. Sprawdzenie liczby słów w każdej recenzji\n",
        "train = trainDF.copy()\n",
        "train['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
        "\n",
        "print(\"Podgląd liczby słów:\")\n",
        "print(train[['text','word_count']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2405622-5b04-4c93-a52e-488d7a7b26c7",
      "metadata": {
        "id": "a2405622-5b04-4c93-a52e-488d7a7b26c7",
        "outputId": "126864ae-5842-415e-9741-ea3f31cb97d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Statystyki liczby słów:\n",
            "463\n",
            "1\n",
            "51.404419916132454\n"
          ]
        }
      ],
      "source": [
        "# Statystyki dla liczby słów (odpowiednik printów z oryginału)\n",
        "print(\"\\nStatystyki liczby słów:\")\n",
        "print(train['word_count'].max())\n",
        "print(train['word_count'].min())\n",
        "print(train['word_count'].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ec7abf-acb9-4934-b02e-6ba74bc28c95",
      "metadata": {
        "id": "85ec7abf-acb9-4934-b02e-6ba74bc28c95",
        "outputId": "60304bf7-ad9e-46ca-97f2-55e55194a863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Podgląd liczby znaków:\n",
            "                                                text  char_count\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...         337\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...          82\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...         284\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...         128\n",
            "4  Мне не очень  понравилось это платье. Размер  ...         120\n"
          ]
        }
      ],
      "source": [
        "# 2. Sprawdzenie liczby znaków (łącznie ze spacjami)\n",
        "train['char_count'] = train['text'].str.len()\n",
        "print(\"\\nPodgląd liczby znaków:\")\n",
        "print(train[['text','char_count']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42248c1-006d-4eb7-a7b7-89005c910e23",
      "metadata": {
        "id": "c42248c1-006d-4eb7-a7b7-89005c910e23",
        "outputId": "8fcfb104-0451-4762-f6bd-ae22a5ed14b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Średnia długość słowa:\n",
            "                                                text  avg_word\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...  5.607843\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...  6.545455\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...  5.130435\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...  5.736842\n",
            "4  Мне не очень  понравилось это платье. Размер  ...  5.555556\n"
          ]
        }
      ],
      "source": [
        "# 3. Średnia długość słowa\n",
        "def avg_word(sentence):\n",
        "  words = str(sentence).split()\n",
        "  if len(words) == 0:\n",
        "    return 0\n",
        "  return (sum(len(word) for word in words)/len(words))\n",
        "\n",
        "train['avg_word'] = train['text'].apply(lambda x: avg_word(x))\n",
        "print(\"\\nŚrednia długość słowa:\")\n",
        "print(train[['text','avg_word']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9858959-87f8-4f40-8de6-2d4d4e544f2d",
      "metadata": {
        "id": "b9858959-87f8-4f40-8de6-2d4d4e544f2d",
        "outputId": "3e2c25c5-ff8e-4853-be9f-0685f3fe9a91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Statystyki stop-words (Top 5):\n",
            "                                                text  stopwords\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...         11\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...          4\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...         14\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...          4\n",
            "4  Мне не очень  понравилось это платье. Размер  ...          3\n"
          ]
        }
      ],
      "source": [
        "# 1. Liczenie stop-words (słów nieistotnych) - dla rosyjskiego!\n",
        "stop = stopwords.words('russian')\n",
        "train['stopwords'] = train['text'].apply(lambda x: len([x for x in str(x).split() if x in stop]))\n",
        "print(\"Statystyki stop-words (Top 5):\")\n",
        "print(train[['text','stopwords']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed14d14e-8c8e-4508-894f-f734ac5b5b94",
      "metadata": {
        "id": "ed14d14e-8c8e-4508-894f-f734ac5b5b94",
        "outputId": "d0fb4932-1fd6-46fd-a958-f7b5876b042a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Statystyki liczb (Top 5):\n",
            "                                                text  numerics\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...         0\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...         0\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...         0\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...         0\n",
            "4  Мне не очень  понравилось это платье. Размер  ...         0\n"
          ]
        }
      ],
      "source": [
        "# 2. Liczenie liczb w tekście\n",
        "train['numerics'] = train['text'].apply(lambda x: len([x for x in str(x).split() if x.isnumeric()]))\n",
        "print(\"\\nStatystyki liczb (Top 5):\")\n",
        "print(train[['text','numerics']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8ae953-0d1c-49e2-8ce2-54edc4372f76",
      "metadata": {
        "id": "5d8ae953-0d1c-49e2-8ce2-54edc4372f76",
        "outputId": "95670590-e0eb-4e86-957f-81e0e515346a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tekst po wstępnym czyszczeniu (małe litery, brak interpunkcji i stop-words):\n",
            "0    пальто красивое пришло дырой молнии просила вы...\n",
            "1    очень долго шел заказждала новому годупришел н...\n",
            "2    могу сказать одно брюки нормальные порваны мал...\n",
            "3    доставка быстрая меньше месяца заказывали разм...\n",
            "4    очень понравилось это платье размер l подошёл ...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# --- CZYSZCZENIE (Preprocessing) ---\n",
        "\n",
        "# Zachowujemy wersję przed czyszczeniem (tak jak w oryginale)\n",
        "trainDFRaw = trainDF.copy()\n",
        "\n",
        "# A. Zamiana na małe litery\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(str(x).lower() for x in str(x).split()))\n",
        "\n",
        "# B. Usuwanie znaków specjalnych i interpunkcji\n",
        "# Używamy regex, który zachowuje litery (w tym cyrylicę) i spacje\n",
        "trainDF['text'] = trainDF['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "\n",
        "# C. Usuwanie rosyjskich stop-words\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop))\n",
        "\n",
        "print(\"\\nTekst po wstępnym czyszczeniu (małe litery, brak interpunkcji i stop-words):\")\n",
        "print(trainDF['text'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c66e9b9-86c1-4c32-b4f4-f49f5e64a707",
      "metadata": {
        "id": "6c66e9b9-86c1-4c32-b4f4-f49f5e64a707",
        "outputId": "c21a7712-b431-4261-95aa-30da6f7f8bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Najczęstsze słowa do usunięcia:\n",
            "это           115810\n",
            "очень         109010\n",
            "просто         41663\n",
            "аниме          35086\n",
            "фильм          31993\n",
            "всё            29624\n",
            "10             25739\n",
            "вообще         22314\n",
            "размер         20762\n",
            "хотя           19585\n",
            "время          18904\n",
            "ещё            18072\n",
            "сюжет          17975\n",
            "деньги         17693\n",
            "качество       17178\n",
            "2              16456\n",
            "смотреть       15528\n",
            "товар          15091\n",
            "рекомендую     14750\n",
            "которые        14686\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# --- USUNIĘCIE SŁÓW CZĘSTYCH I RZADKICH ORAZ LEMATYZACJA ---\n",
        "\n",
        "# 1. Przygotowanie listy 20 najczęstszych słów (Top 20 Frequent Words)\n",
        "freq = pd.Series(' '.join(trainDF['text']).split()).value_counts()[:20]\n",
        "print(\"Najczęstsze słowa do usunięcia:\")\n",
        "print(freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a927b517-61d3-4220-ad44-29fe58504d20",
      "metadata": {
        "id": "a927b517-61d3-4220-ad44-29fe58504d20"
      },
      "outputs": [],
      "source": [
        "# Usuwanie najczęstszych słów\n",
        "freq_list = list(freq.index)\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c914197-7759-49de-96a4-76522e8aedf9",
      "metadata": {
        "id": "8c914197-7759-49de-96a4-76522e8aedf9"
      },
      "outputs": [],
      "source": [
        "# 2. Przygotowanie listy 20 najrzadszych słów (Top 20 Rare Words)\n",
        "# rare = pd.Series(' '.join(trainDF['text']).split()).value_counts()[-20:]\n",
        "# print(\"\\nNajrzadsze słowa do usunięcia:\")\n",
        "# print(rare) - to podejście się nie sprawdza, jest to 20 losowych słów, przy tak dużym zbiorze jest więcej słów, które występują tylko raz. HAPAX LEGOMENA\n",
        "# Usuwanie najrzadszych słów\n",
        "# rare_list = list(rare.index)\n",
        "# trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8818b4ee-9cda-4d26-b0e8-0a31e0f7b054",
      "metadata": {
        "id": "8818b4ee-9cda-4d26-b0e8-0a31e0f7b054",
        "outputId": "6d3ec96c-091a-48dd-893e-3823bdaa422e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Liczba unikalnych słów do usunięcia (występujących tylko raz): 814773\n"
          ]
        }
      ],
      "source": [
        "# 1. Logika lingwistyczna: Usuwanie wszystkich słów występujących tylko RAZ\n",
        "all_words = pd.Series(' '.join(trainDF['text']).split())\n",
        "word_counts = all_words.value_counts()\n",
        "rare_words_logical = word_counts[word_counts == 1].index\n",
        "\n",
        "print(f\"Liczba unikalnych słów do usunięcia (występujących tylko raz): {len(rare_words_logical)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57d5cab-0ab1-4637-8908-868b10c3f453",
      "metadata": {
        "id": "f57d5cab-0ab1-4637-8908-868b10c3f453"
      },
      "outputs": [],
      "source": [
        "# Usuwamy je\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare_words_logical))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557efa69-3df4-4a87-84b9-f86f4294b40d",
      "metadata": {
        "id": "557efa69-3df4-4a87-84b9-f86f4294b40d"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "from collections import namedtuple\n",
        "\n",
        "# Definiujemy strukturę, którą zrozumie stary kod pymorphy2\n",
        "ArgSpec = namedtuple('ArgSpec', ['args', 'varargs', 'keywords', 'defaults'])\n",
        "\n",
        "def getargspec_patch(func):\n",
        "    args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations = inspect.getfullargspec(func)\n",
        "    return ArgSpec(args, varargs, varkw, defaults)\n",
        "\n",
        "# Podmieniamy funkcję w module inspect\n",
        "inspect.getargspec = getargspec_patch\n",
        "\n",
        "import pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Teraz ten kod powinien przejść bez błędu ValueError:\n",
        "sample_text = \"пальто пришло красивое\"\n",
        "lemmas = [morph.parse(word)[0].normal_form for word in sample_text.split()]\n",
        "print(lemmas) # Powinno wyjść: ['пальто', 'прийти', 'красивый']\n",
        "import inspect\n",
        "# Naprawa błędu zgodności dla Python 3.11+ i pymorphy2\n",
        "if not hasattr(inspect, 'getargspec'):\n",
        "    inspect.getargspec = inspect.getfullargspec\n",
        "\n",
        "# 2. PORÓWNANIE LEMATYZACJI (na próbce 5 pierwszych recenzji)\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "mystem = Mystem()\n",
        "\n",
        "sample_texts = trainDF['text'].head(5).tolist()\n",
        "\n",
        "print(\"\\n--- PORÓWNANIE LEMATYZACJI ---\")\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"\\nOryginał: {text[:100]}...\")\n",
        "\n",
        "    # Pymorphy2\n",
        "    py_lemmas = [morph.parse(word)[0].normal_form for word in text.split()]\n",
        "    print(f\"Pymorphy2: {' '.join(py_lemmas)[:100]}...\")\n",
        "\n",
        "    # Mystem\n",
        "    myst_lemmas = mystem.lemmatize(text)\n",
        "    print(f\"Mystem: {''.join(myst_lemmas).strip()[:100]}...\")\n",
        "\n",
        "# --- WYBÓR DO PROJEKTU: Mystem (wybrany ze względu na lepszą jakość w testach) ---\n",
        "print(\"Uruchamiam wydajną lematyzację Mystem dla całego zbioru (290k rekordów)...\")\n",
        "\n",
        "# Metoda wydajnego przetwarzania dużych tekstów przez Mystem\n",
        "def mystem_full_process(texts_list):\n",
        "    # Łączymy wszystkie teksty specjalnym separatorem, aby Mystem przetworzył je jako jeden ciąg\n",
        "    full_text = \" |separator| \".join(texts_list)\n",
        "    lemmatized = mystem.lemmatize(full_text)\n",
        "\n",
        "    # Składamy z powrotem w listę oczyszczonych tekstów\n",
        "    processed_text = \"\".join(lemmatized)\n",
        "    return [t.strip() for t in processed_text.split(\"|separator|\")]\n",
        "\n",
        "# Przetwarzanie w paczkach po 5000 wierszy, aby nie przepełnić pamięci\n",
        "all_texts = trainDF['text'].tolist()\n",
        "final_lemmas = []\n",
        "batch_size = 5000\n",
        "\n",
        "for i in range(0, len(all_texts), batch_size):\n",
        "    batch = all_texts[i:i + batch_size]\n",
        "    final_lemmas.extend(mystem_full_process(batch))\n",
        "    if (i // batch_size) % 5 == 0:\n",
        "        print(f\"Przetworzono {i} / {len(all_texts)} rekordów...\")\n",
        "\n",
        "trainDF['text'] = final_lemmas\n",
        "\n",
        "print(\"\\nPełna lematyzacja Mystem zakończona. Podgląd wyników:\")\n",
        "print(trainDF['text'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39345983-a25d-4f1d-8a7c-8a3136cac002",
      "metadata": {
        "id": "39345983-a25d-4f1d-8a7c-8a3136cac002"
      },
      "outputs": [],
      "source": [
        "# 1. Podział na zbiór treningowy i walidacyjny\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# 2. Kodowanie etykiet\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.transform(valid_y)\n",
        "\n",
        "# 3. Wektoryzacja TF-IDF\n",
        "# min_df=2 -> automatycznie ignoruje 814k rzadkich słów\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000, min_df=2)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "\n",
        "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
        "\n",
        "print(\"Przygotowanie danych zakończone.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83aa83d-c830-477a-9933-cd05783e2326",
      "metadata": {
        "id": "c83aa83d-c830-477a-9933-cd05783e2326",
        "outputId": "d8a09352-55da-41ce-ef8d-181060d0d071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR, TF-IDF Accuracy:  0.6834813743716863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\PlumResearch\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM, TF-IDF Accuracy:  0.6801625008607037\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[36], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVM, TF-IDF Accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, results_svm[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# MODEL 3 - Random Forest (Las Losowy)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m results_rf \u001b[38;5;241m=\u001b[39m train_model(ensemble\u001b[38;5;241m.\u001b[39mRandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n\u001b[0;32m     35\u001b[0m accuracy_compare[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m results_rf\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRF, TF-IDF Accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, results_rf[\u001b[38;5;241m3\u001b[39m])\n",
            "Cell \u001b[1;32mIn[36], line 6\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(classifier, feature_vector_train, label, feature_vector_valid)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(classifier, feature_vector_train, label, feature_vector_valid):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Trenowanie modelu\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mfit(feature_vector_train, label)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Generowanie przewidywań dla zbioru testowego\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(feature_vector_valid)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    492\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    493\u001b[0m )(\n\u001b[0;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    495\u001b[0m         t,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    497\u001b[0m         X,\n\u001b[0;32m    498\u001b[0m         y,\n\u001b[0;32m    499\u001b[0m         sample_weight,\n\u001b[0;32m    500\u001b[0m         i,\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    502\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    503\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    504\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    193\u001b[0m         X,\n\u001b[0;32m    194\u001b[0m         y,\n\u001b[0;32m    195\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight,\n\u001b[0;32m    196\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    197\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# --- TRENOWANIE I EWALUACJA MODELI ---\n",
        "\n",
        "# 1. Uniwersalna funkcja do trenowania (Odwzorowanie 1:1 z oryginału)\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    # Trenowanie modelu\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "\n",
        "    # Generowanie przewidywań dla zbioru testowego\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "\n",
        "    # Obliczanie metryk (Precision, Recall, F1, Accuracy)\n",
        "    precision = metrics.precision_score(predictions, valid_y, average='weighted')\n",
        "    recall = metrics.recall_score(predictions, valid_y, average='weighted')\n",
        "    f1 = metrics.f1_score(predictions, valid_y, average='weighted')\n",
        "    accuracy = metrics.accuracy_score(predictions, valid_y)\n",
        "\n",
        "    return [precision, recall, f1, accuracy]\n",
        "\n",
        "# Słownik do przechowywania wyników dla porównania\n",
        "accuracy_compare = {}\n",
        "\n",
        "# MODEL 1 - Regresja Logistyczna (Logistic Regression)\n",
        "results_lr = train_model(linear_model.LogisticRegression(max_iter=1000), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "accuracy_compare['LR'] = results_lr\n",
        "print (\"LR, TF-IDF Accuracy: \", results_lr[3])\n",
        "\n",
        "# MODEL 2 - Support Vector Machine (SVM)\n",
        "# Uwaga: Na tak dużym zbiorze SVM może zająć kilka minut\n",
        "results_svm = train_model(svm.LinearSVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "accuracy_compare['SVM'] = results_svm\n",
        "print (\"SVM, TF-IDF Accuracy: \", results_svm[3])\n",
        "\n",
        "# MODEL 3 - Random Forest (Las Losowy)\n",
        "results_rf = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "accuracy_compare['RF'] = results_rf\n",
        "print (\"RF, TF-IDF Accuracy: \", results_rf[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4123c81-f234-405e-89bd-b71b2773348b",
      "metadata": {
        "id": "b4123c81-f234-405e-89bd-b71b2773348b"
      },
      "outputs": [],
      "source": [
        "# --- ZAPISYWANIE MODELI DO PLIKÓW ---\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Tworzymy folder na modele, jeśli nie istnieje\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "# Zapisujemy każdy model do oddzielnego pliku\n",
        "joblib.dump(accuracy_compare, 'models/results_dict.pkl')\n",
        "joblib.dump(tfidf_vect, 'models/tfidf_vectorizer.pkl')\n",
        "\n",
        "# Jeśli modele skończyły się trenować, zapisujemy je (pamiętaj, że RF może jeszcze trwać)\n",
        "if 'LR' in accuracy_compare:\n",
        "    # Zapisujemy same obiekty modeli (można je później wczytać do predykcji)\n",
        "    # Uwaga: poniższe zmienne muszą istnieć (musisz mieć skończony trening)\n",
        "    # joblib.dump(classifier_lr, 'models/model_lr.pkl') # jeśli zapisałaś je w zmiennych\n",
        "\n",
        "    print(\"Wyniki i wektoryzator zostały zapisane w folderze 'models'.\")\n",
        "    print(\"Teraz nawet po zamknięciu laptopa, odtworzysz je w sekundę.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d978f1f-e6f5-439e-b5d7-7f421d12d3b1",
      "metadata": {
        "id": "9d978f1f-e6f5-439e-b5d7-7f421d12d3b1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}