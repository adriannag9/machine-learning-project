{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adriannag9/machine-learning/blob/main/ru_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f36267-d8b3-4d9a-984c-dba7462dca47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37f36267-d8b3-4d9a-984c-dba7462dca47",
        "outputId": "ac41c5c7-c5aa-43df-87cc-43483e1e77bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.12/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pymystem3 in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.12/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.12/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pymystem3) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2026.1.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                                                 text  label\n",
            "0   Пальто красивое, но пришло с дырой в молнии. П...      0\n",
            "1   Очень долго шел заказ,ждала к новому году,приш...      0\n",
            "2   Могу сказать одно, брюки нормальные, НО они бы...      0\n",
            "3   Доставка быстрая, меньше месяца. Заказывали ра...      0\n",
            "4   Мне не очень  понравилось это платье. Размер  ...      0\n",
            "..                                                ...    ...\n",
            "95  Доставка 63 дня!!! Качество хорошее, на ОГ95, ...      0\n",
            "96  мой размер одежды 40, поэтому думаю понимаете,...      0\n",
            "97  Покрой халата очень странный, рукава очень дли...      0\n",
            "98  заказывала две с длинным рукавом, одна пришла ...      0\n",
            "99         не соответствует фото.Качество другое. :((      0\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "               label\n",
            "count  290458.000000\n",
            "mean        1.001387\n",
            "std         0.816375\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%         1.000000\n",
            "75%         2.000000\n",
            "max         2.000000\n"
          ]
        }
      ],
      "source": [
        "# Install required Russian NLP libraries\n",
        "!pip install pymorphy2 pymystem3\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, preprocessing, linear_model, metrics, svm\n",
        "from sklearn import ensemble\n",
        "import pymorphy2\n",
        "from pymystem3 import Mystem\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Mount Google Drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "# Path adjusted for Google Drive storage\n",
        "raw_data = pd.read_csv('/content/drive/My Drive/projekt_ml/sentiment_dataset.csv')\n",
        "\n",
        "# Prepare data lists\n",
        "texts = raw_data['text'].astype(str).tolist()\n",
        "labels = raw_data['label'].tolist()\n",
        "\n",
        "# Create main DataFrame\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "\n",
        "# Display initial data statistics and preview\n",
        "print(trainDF.head(100))\n",
        "print(trainDF.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34a7b82f-b98f-4211-840b-260e1b7f6a00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34a7b82f-b98f-4211-840b-260e1b7f6a00",
        "outputId": "0fbeb95b-cd6f-4406-ed16-ac7bb030a36b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word count preview:\n",
            "                                                text  word_count\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...          50\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...          11\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...          49\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...          20\n",
            "4  Мне не очень  понравилось это платье. Размер  ...          21\n"
          ]
        }
      ],
      "source": [
        "# DATA ANALYSIS (Exploratory Data Analysis)\n",
        "# 1. Word count analysis\n",
        "train = trainDF.copy()\n",
        "train['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\n",
        "\n",
        "print(\"Word count preview:\")\n",
        "print(train[['text','word_count']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2405622-5b04-4c93-a52e-488d7a7b26c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2405622-5b04-4c93-a52e-488d7a7b26c7",
        "outputId": "0ab1c5c4-4d2e-41d4-fbbd-3738ffb0d6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Word count statistics:\n",
            "Max: 463\n",
            "Min: 1\n",
            "Mean: 51.404419916132454\n"
          ]
        }
      ],
      "source": [
        "# Word count statistics\n",
        "print(\"\\nWord count statistics:\")\n",
        "print(f\"Max: {train['word_count'].max()}\")\n",
        "print(f\"Min: {train['word_count'].min()}\")\n",
        "print(f\"Mean: {train['word_count'].mean()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ec7abf-acb9-4934-b02e-6ba74bc28c95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85ec7abf-acb9-4934-b02e-6ba74bc28c95",
        "outputId": "52c51995-9dac-47da-edd0-2484cd6ed36a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Character count preview:\n",
            "                                                text  char_count\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...         337\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...          82\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...         284\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...         128\n",
            "4  Мне не очень  понравилось это платье. Размер  ...         120\n"
          ]
        }
      ],
      "source": [
        "# 2. Character count analysis (including spaces)\n",
        "train['char_count'] = train['text'].str.len()\n",
        "print(\"\\nCharacter count preview:\")\n",
        "print(train[['text','char_count']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42248c1-006d-4eb7-a7b7-89005c910e23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c42248c1-006d-4eb7-a7b7-89005c910e23",
        "outputId": "87b11b16-f6c3-428d-cd11-ad4e753b2bdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average word length preview:\n",
            "                                                text  avg_word\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...  5.607843\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...  6.545455\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...  5.130435\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...  5.736842\n",
            "4  Мне не очень  понравилось это платье. Размер  ...  5.555556\n"
          ]
        }
      ],
      "source": [
        "# 3. Average word length analysis\n",
        "def avg_word(sentence):\n",
        "    words = str(sentence).split()\n",
        "    if len(words) == 0:\n",
        "        return 0\n",
        "    return (sum(len(word) for word in words)/len(words))\n",
        "\n",
        "train['avg_word'] = train['text'].apply(lambda x: avg_word(x))\n",
        "print(\"\\nAverage word length preview:\")\n",
        "print(train[['text','avg_word']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9858959-87f8-4f40-8de6-2d4d4e544f2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9858959-87f8-4f40-8de6-2d4d4e544f2d",
        "outputId": "e0a7898d-a076-4dbc-b101-199c8899bc2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop-words statistics (Top 5):\n",
            "                                                text  stopwords_count\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...               11\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...                4\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...               14\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...                4\n",
            "4  Мне не очень  понравилось это платье. Размер  ...                3\n",
            "Stopwords:  ['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ],
      "source": [
        "# 1. Count Russian stop-words\n",
        "# Defining the list of Russian stop-words for filtering\n",
        "stop_words = stopwords.words('russian')\n",
        "train['stopwords_count'] = train['text'].apply(lambda x: len([word for word in str(x).split() if word in stop_words]))\n",
        "\n",
        "print(\"Stop-words statistics (Top 5):\")\n",
        "print(train[['text','stopwords_count']].head())\n",
        "print(\"Stopwords: \", stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed14d14e-8c8e-4508-894f-f734ac5b5b94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed14d14e-8c8e-4508-894f-f734ac5b5b94",
        "outputId": "6037f94c-835b-4c9b-a594-0b0b6076b209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Numeric values statistics (Top 5):\n",
            "                                                text  numerics_count\n",
            "0  Пальто красивое, но пришло с дырой в молнии. П...               0\n",
            "1  Очень долго шел заказ,ждала к новому году,приш...               0\n",
            "2  Могу сказать одно, брюки нормальные, НО они бы...               0\n",
            "3  Доставка быстрая, меньше месяца. Заказывали ра...               0\n",
            "4  Мне не очень  понравилось это платье. Размер  ...               0\n"
          ]
        }
      ],
      "source": [
        "# 2. Count numeric values in text\n",
        "# This helps identify if numeric data (like prices or dates) impacts sentiment\n",
        "train['numerics_count'] = train['text'].apply(lambda x: len([word for word in str(x).split() if word.isnumeric()]))\n",
        "\n",
        "print(\"\\nNumeric values statistics (Top 5):\")\n",
        "print(train[['text','numerics_count']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8ae953-0d1c-49e2-8ce2-54edc4372f76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d8ae953-0d1c-49e2-8ce2-54edc4372f76",
        "outputId": "0da4fa17-e1b1-476f-eff1-9a6b16c33c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Text after initial preprocessing (lowercase, no punctuation, stop-words removed):\n",
            "0    пальто красивое пришло дырой молнии просила вы...\n",
            "1    очень долго шел заказждала новому годупришел н...\n",
            "2    могу сказать одно брюки нормальные порваны мал...\n",
            "3    доставка быстрая меньше месяца заказывали разм...\n",
            "4    очень понравилось это платье размер l подошёл ...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# TEXT PREPROCESSING\n",
        "\n",
        "# Keep a copy of the raw data for reference\n",
        "trainDF_raw = trainDF.copy()\n",
        "\n",
        "# A. Lowercase conversion\n",
        "# Normalizing text to lowercase for consistency\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(str(x).lower() for x in str(x).split()))\n",
        "\n",
        "# B. Removing special characters and punctuation\n",
        "# Using regex to keep only alphanumeric characters and spaces\n",
        "trainDF['text'] = trainDF['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "\n",
        "# C. Removing Russian stop-words\n",
        "# Filtering out non-informative words based on the NLTK Russian stop-words list\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(word for word in str(x).split() if word not in stop_words))\n",
        "\n",
        "print(\"\\nText after initial preprocessing (lowercase, no punctuation, stop-words removed):\")\n",
        "print(trainDF['text'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c66e9b9-86c1-4c32-b4f4-f49f5e64a707",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c66e9b9-86c1-4c32-b4f4-f49f5e64a707",
        "outputId": "75f379a3-990a-4936-d7a7-ce747898812b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 20 most frequent words identified for removal:\n",
            "это           115810\n",
            "очень         109010\n",
            "просто         41663\n",
            "аниме          35086\n",
            "фильм          31993\n",
            "всё            29624\n",
            "10             25739\n",
            "вообще         22314\n",
            "размер         20762\n",
            "хотя           19585\n",
            "время          18904\n",
            "ещё            18072\n",
            "сюжет          17975\n",
            "деньги         17693\n",
            "качество       17178\n",
            "2              16456\n",
            "смотреть       15528\n",
            "товар          15091\n",
            "рекомендую     14750\n",
            "которые        14686\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# REMOVING FREQUENT AND RARE WORDS\n",
        "\n",
        "# 1. Identify the top 20 most frequent words\n",
        "# These words often appear across all classes and may not contribute to sentiment differentiation\n",
        "freq_words = pd.Series(' '.join(trainDF['text']).split()).value_counts()[:20]\n",
        "\n",
        "print(\"Top 20 most frequent words identified for removal:\")\n",
        "print(freq_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a927b517-61d3-4220-ad44-29fe58504d20",
      "metadata": {
        "id": "a927b517-61d3-4220-ad44-29fe58504d20"
      },
      "outputs": [],
      "source": [
        "# Remove the most frequent words\n",
        "frequent_words_list = list(freq_words.index)\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(word for word in str(x).split() if word not in frequent_words_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8818b4ee-9cda-4d26-b0e8-0a31e0f7b054",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8818b4ee-9cda-4d26-b0e8-0a31e0f7b054",
        "outputId": "954ef062-da4f-46c6-c3c8-0ae12629d74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Hapax Legomena to be removed: 425023\n",
            "Rare words removal completed.\n"
          ]
        }
      ],
      "source": [
        "# 2. Identifying and Removing Rare Words (Hapax Legomena)\n",
        "# For a large dataset (~300k records), removing only the bottom 20 words is insufficient.\n",
        "# Instead, we identify \"Hapax Legomena\" — words that appear only once in the entire corpus.\n",
        "\n",
        "# Calculate frequency of all words\n",
        "word_freq = pd.Series(' '.join(trainDF['text']).split()).value_counts()\n",
        "\n",
        "# Identify words with a frequency of 1\n",
        "rare_words = word_freq[word_freq == 1]\n",
        "print(f\"Number of Hapax Legomena to be removed: {len(rare_words)}\")\n",
        "\n",
        "# Remove rare words to reduce noise and improve model generalization\n",
        "# Using a set for significantly faster lookup performance\n",
        "rare_words_set = set(rare_words.index)\n",
        "trainDF['text'] = trainDF['text'].apply(lambda x: \" \".join(word for word in str(x).split() if word not in rare_words_set))\n",
        "\n",
        "print(\"Rare words removal completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557efa69-3df4-4a87-84b9-f86f4294b40d",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "557efa69-3df4-4a87-84b9-f86f4294b40d",
        "outputId": "247781ba-6217-43b5-ace4-e57170032ee7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- LEMMATIZATION COMPARISON ---\n",
            "\n",
            "Original: пальто красивое пришло дырой молнии просила выслать такую продавец настаивал отк...\n",
            "Pymorphy2: пальто красивый прийти дыра молния просить выслать такой продавец настаивать отк...\n",
            "Mystem:    пальто красивый приходить дыра молния просить высылать такой продавец настаивать...\n",
            "\n",
            "Original: долго шел заказждала новому...\n",
            "Pymorphy2: долго идти заказждать новый...\n",
            "Mystem:    долго идти заказждать новый...\n",
            "\n",
            "Original: могу сказать одно брюки нормальные порваны малы заказывала второй заказала пришё...\n",
            "Pymorphy2: мочь сказать один брюки нормальный порвать маленький заказывать второй заказать ...\n",
            "Mystem:    мочь сказать один брюки нормальный порывать маленький заказывать второй заказыва...\n",
            "\n",
            "Original: доставка быстрая меньше месяца заказывали l пришёл среднее края обработаны такие...\n",
            "Pymorphy2: доставка быстрый маленький месяц заказывать l прийти средний край обработать так...\n",
            "Mystem:    доставка быстрый мало месяц заказывать l приходить средний край обрабатывать так...\n",
            "\n",
            "Original: понравилось платье l подошёл зелёного цвета смотрится пришло быстро спасибо...\n",
            "Pymorphy2: понравиться платье l подойти зелёный цвет смотреться прийти быстро спасибо...\n",
            "Mystem:    понравиться платье l подходить зеленый цвет смотреться приходить быстро спасибо...\n",
            "\n",
            "Starting efficient Mystem lemmatization for the full dataset (~300k records)...\n",
            "Progress: 0 / 290458 records processed...\n",
            "Progress: 50000 / 290458 records processed...\n",
            "Progress: 100000 / 290458 records processed...\n",
            "Progress: 150000 / 290458 records processed...\n",
            "Progress: 200000 / 290458 records processed...\n",
            "Progress: 250000 / 290458 records processed...\n",
            "\n",
            "Full Mystem lemmatization completed. Data preview:\n",
            "0    пальто красивый приходить дыра молния просить ...\n",
            "1                          долго идти заказждать новый\n",
            "2    мочь сказать один брюки нормальный порывать ма...\n",
            "3    доставка быстрый мало месяц заказывать l прихо...\n",
            "4    понравиться платье l подходить зеленый цвет см...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# LEMMATIZATION STRATEGY: COMPARISON AND FINAL PROCESSING -\n",
        "\n",
        "import inspect\n",
        "from collections import namedtuple\n",
        "\n",
        "# 1. Compatibility Patch for pymorphy2 (Required for Python 3.11+)\n",
        "# Fixing the 'getargspec' attribute error in modern Python environments\n",
        "if not hasattr(inspect, 'getargspec'):\n",
        "    ArgSpec = namedtuple('ArgSpec', ['args', 'varargs', 'keywords', 'defaults'])\n",
        "    def getargspec_patch(func):\n",
        "        args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations = inspect.getfullargspec(func)\n",
        "        return ArgSpec(args, varargs, varkw, defaults)\n",
        "    inspect.getargspec = getargspec_patch\n",
        "\n",
        "# Initialize lemmatizers\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "mystem = Mystem()\n",
        "\n",
        "# 2. Lemmatization Comparison (Sample Test)\n",
        "# Testing Pymorphy2 vs Mystem on a small sample to justify the final choice\n",
        "sample_texts = trainDF['text'].head(5).tolist()\n",
        "\n",
        "print(\"--- LEMMATIZATION COMPARISON ---\")\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"\\nOriginal: {text[:80]}...\")\n",
        "\n",
        "    # Pymorphy2 approach\n",
        "    py_lemmas = [morph.parse(word)[0].normal_form for word in text.split()]\n",
        "    print(f\"Pymorphy2: {' '.join(py_lemmas)[:80]}...\")\n",
        "\n",
        "    # Mystem approach\n",
        "    myst_lemmas = mystem.lemmatize(text)\n",
        "    print(f\"Mystem:    {''.join(myst_lemmas).strip()[:80]}...\")\n",
        "\n",
        "# 3. Final Lemmatization: Mystem (Selected for superior accuracy)\n",
        "print(\"\\nStarting efficient Mystem lemmatization for the full dataset (~300k records)...\")\n",
        "\n",
        "def mystem_batch_process(texts_list):\n",
        "    \"\"\"Efficiently processes large text lists by joining them with a separator.\"\"\"\n",
        "    full_text = \" |separator| \".join(texts_list)\n",
        "    lemmatized = mystem.lemmatize(full_text)\n",
        "    processed_text = \"\".join(lemmatized)\n",
        "    return [t.strip() for t in processed_text.split(\"|separator|\")]\n",
        "\n",
        "# Processing in batches of 5000 to optimize memory usage in Google Colab\n",
        "all_texts = trainDF['text'].tolist()\n",
        "final_lemmas = []\n",
        "batch_size = 5000\n",
        "\n",
        "for i in range(0, len(all_texts), batch_size):\n",
        "    batch = all_texts[i:i + batch_size]\n",
        "    final_lemmas.extend(mystem_batch_process(batch))\n",
        "    if (i // batch_size) % 10 == 0:\n",
        "        print(f\"Progress: {i} / {len(all_texts)} records processed...\")\n",
        "\n",
        "# Update DataFrame with lemmatized text\n",
        "trainDF['text'] = final_lemmas\n",
        "\n",
        "print(\"\\nFull Mystem lemmatization completed. Data preview:\")\n",
        "print(trainDF['text'].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39345983-a25d-4f1d-8a7c-8a3136cac002",
      "metadata": {
        "id": "39345983-a25d-4f1d-8a7c-8a3136cac002"
      },
      "outputs": [],
      "source": [
        "# 1. Podział na zbiór treningowy i walidacyjny\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# 2. Kodowanie etykiet\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.transform(valid_y)\n",
        "\n",
        "# 3. Wektoryzacja TF-IDF\n",
        "# min_df=2 -> automatycznie ignoruje 814k rzadkich słów\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000, min_df=2)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "\n",
        "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf = tfidf_vect.transform(valid_x)\n",
        "\n",
        "print(\"Przygotowanie danych zakończone.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83aa83d-c830-477a-9933-cd05783e2326",
      "metadata": {
        "id": "c83aa83d-c830-477a-9933-cd05783e2326",
        "outputId": "d8a09352-55da-41ce-ef8d-181060d0d071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR, TF-IDF Accuracy:  0.6834813743716863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\PlumResearch\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM, TF-IDF Accuracy:  0.6801625008607037\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[36], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVM, TF-IDF Accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, results_svm[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# MODEL 3 - Random Forest (Las Losowy)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m results_rf \u001b[38;5;241m=\u001b[39m train_model(ensemble\u001b[38;5;241m.\u001b[39mRandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n\u001b[0;32m     35\u001b[0m accuracy_compare[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRF\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m results_rf\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRF, TF-IDF Accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, results_rf[\u001b[38;5;241m3\u001b[39m])\n",
            "Cell \u001b[1;32mIn[36], line 6\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(classifier, feature_vector_train, label, feature_vector_valid)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(classifier, feature_vector_train, label, feature_vector_valid):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Trenowanie modelu\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mfit(feature_vector_train, label)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Generowanie przewidywań dla zbioru testowego\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(feature_vector_valid)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    492\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    493\u001b[0m )(\n\u001b[0;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    495\u001b[0m         t,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    497\u001b[0m         X,\n\u001b[0;32m    498\u001b[0m         y,\n\u001b[0;32m    499\u001b[0m         sample_weight,\n\u001b[0;32m    500\u001b[0m         i,\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    502\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    503\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    504\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    193\u001b[0m         X,\n\u001b[0;32m    194\u001b[0m         y,\n\u001b[0;32m    195\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight,\n\u001b[0;32m    196\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    197\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
            "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# --- TRENOWANIE I EWALUACJA MODELI ---\n",
        "\n",
        "# 1. Uniwersalna funkcja do trenowania (Odwzorowanie 1:1 z oryginału)\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    # Trenowanie modelu\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "\n",
        "    # Generowanie przewidywań dla zbioru testowego\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "\n",
        "    # Obliczanie metryk (Precision, Recall, F1, Accuracy)\n",
        "    precision = metrics.precision_score(predictions, valid_y, average='weighted')\n",
        "    recall = metrics.recall_score(predictions, valid_y, average='weighted')\n",
        "    f1 = metrics.f1_score(predictions, valid_y, average='weighted')\n",
        "    accuracy = metrics.accuracy_score(predictions, valid_y)\n",
        "\n",
        "    return [precision, recall, f1, accuracy]\n",
        "\n",
        "# Słownik do przechowywania wyników dla porównania\n",
        "accuracy_compare = {}\n",
        "\n",
        "# MODEL 1 - Regresja Logistyczna (Logistic Regression)\n",
        "results_lr = train_model(linear_model.LogisticRegression(max_iter=1000), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "accuracy_compare['LR'] = results_lr\n",
        "print (\"LR, TF-IDF Accuracy: \", results_lr[3])\n",
        "\n",
        "# MODEL 2 - Support Vector Machine (SVM)\n",
        "# Uwaga: Na tak dużym zbiorze SVM może zająć kilka minut\n",
        "results_svm = train_model(svm.LinearSVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "accuracy_compare['SVM'] = results_svm\n",
        "print (\"SVM, TF-IDF Accuracy: \", results_svm[3])\n",
        "\n",
        "# MODEL 3 - Random Forest (Las Losowy)\n",
        "results_rf = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "accuracy_compare['RF'] = results_rf\n",
        "print (\"RF, TF-IDF Accuracy: \", results_rf[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4123c81-f234-405e-89bd-b71b2773348b",
      "metadata": {
        "id": "b4123c81-f234-405e-89bd-b71b2773348b"
      },
      "outputs": [],
      "source": [
        "# --- ZAPISYWANIE MODELI DO PLIKÓW ---\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Tworzymy folder na modele, jeśli nie istnieje\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "# Zapisujemy każdy model do oddzielnego pliku\n",
        "joblib.dump(accuracy_compare, 'models/results_dict.pkl')\n",
        "joblib.dump(tfidf_vect, 'models/tfidf_vectorizer.pkl')\n",
        "\n",
        "# Jeśli modele skończyły się trenować, zapisujemy je (pamiętaj, że RF może jeszcze trwać)\n",
        "if 'LR' in accuracy_compare:\n",
        "    # Zapisujemy same obiekty modeli (można je później wczytać do predykcji)\n",
        "    # Uwaga: poniższe zmienne muszą istnieć (musisz mieć skończony trening)\n",
        "    # joblib.dump(classifier_lr, 'models/model_lr.pkl') # jeśli zapisałaś je w zmiennych\n",
        "\n",
        "    print(\"Wyniki i wektoryzator zostały zapisane w folderze 'models'.\")\n",
        "    print(\"Teraz nawet po zamknięciu laptopa, odtworzysz je w sekundę.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d978f1f-e6f5-439e-b5d7-7f421d12d3b1",
      "metadata": {
        "id": "9d978f1f-e6f5-439e-b5d7-7f421d12d3b1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}